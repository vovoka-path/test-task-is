"""
–ì–ª–∞–≤–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–°–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
"""

import json
import logging
import sys
from pathlib import Path
from typing import List

from src.schemas import Chunk

# –ü—É—Ç—å –∫ –¥–æ–∫—É–º–µ–Ω—Ç—É
input_file_path_str = "data/input/1. –ü—Ä–∞–≤–∏–ª–∞ ‚Ññ 32 —Å 11.12.2023.docx"
# –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É
output_file_path_str = "data/output/processed_chunks.json"

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('app.log', mode='a', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


def save_chunks_to_json(chunks: List[Chunk], output_path: Path) -> None:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –≤ JSON —Ñ–∞–π–ª.
    
    Args:
        chunks: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        output_path: –ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É
    """
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ Pydantic-–æ–±—ä–µ–∫—Ç–æ–≤ Chunk –≤ —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π
    chunks_data = [chunk.model_dump() for chunk in chunks]
    
    # –ó–∞–ø–∏—Å—å –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks_data, f, indent=4, ensure_ascii=False)


if __name__ == "__main__":
    try:
        # –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—É—Ç—è–º–∏ –∏ —Å—Ö–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö
        from src.pipeline import run_pipeline
        from src.schemas import Chunk
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—É—Ç–∏ –∫ —Ç–µ—Å—Ç–æ–≤–æ–º—É —Ñ–∞–π–ª—É
        # input_file_path: Path = Path("data/input/test_rules.docx")
        input_file_path: Path = Path(input_file_path_str)
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π print –æ –Ω–∞—á–∞–ª–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        print(f"üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞: {input_file_path}")
        print("=" * 60)
        
        # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        chunks: List[Chunk] = run_pipeline(str(input_file_path))
        
        # –í—ã–≤–æ–¥ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        print(f"‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω. –ü–æ–ª—É—á–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
        print("=" * 60)
        
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä–≤—ã–º 3 —á–∞–Ω–∫–∞–º –¥–ª—è —Ä—É—á–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
        print("üìã –ü–ï–†–í–´–ï 3 –ß–ê–ù–ö–ê –î–õ–Ø –ü–†–û–í–ï–†–ö–ò:")
        print("=" * 60)
        
        for i, chunk in enumerate(chunks[:3]):
            print(f"\n--- –ß–∞–Ω–∫ ‚Ññ{i+1} ---")
            print(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ page_content:")
            print(f"{chunk.page_content}")
            print(f"\n–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ metadata:")
            for key, value in chunk.metadata.model_dump().items():
                print(f"  {key}: {value}")
            print("-" * 40)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –≤ JSON —Ñ–∞–π–ª
        output_file_path = Path(output_file_path_str)
        save_chunks_to_json(chunks, output_file_path)
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_file_path}")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞: {e}")
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –ø–∞–π–ø–ª–∞–π–Ω–µ: {e}", exc_info=True)